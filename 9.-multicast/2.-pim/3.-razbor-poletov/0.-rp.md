
**1\)** Клиент 1 отправляет IGMP Report для группы 224.2.2.4

![Подключение мультикастового клиента](http://img-fotki.yandex.ru/get/6727/83739833.37/0_da313_3e5a9f37_XXL.png)

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**2\)** R4 получает этот запрос, понимает, что есть клиент за интерфейсом FE0/0, добавляет этот интерфейс в OIL и формирует запись \(\*, G\).

![\(\*, G\)](http://img-fotki.yandex.ru/get/9491/83739833.37/0_da312_867e4e37_XL.png)

> Здесь видно восходящий интерфейс FE0/1, но это не значит, что R4 получает трафик для группы 224.2.2.4. Это говорит лишь о том, что единственное место, откуда сейчас он может получать — FE0/1, потому что именно там находится RP. Кстати, здесь же указан и сосед, который прошёл **RPF-Check** — R2: 10.0.2.24. Ожидаемо.

R4 называется — **LHR \(Last Hop Router\)** — последний маршрутизатор на пути мультикастового трафика, если считать от источника. Иными словами — это маршрутизатор, ближайший к получателю. Для _Клиента1_ — это R4, для _Клиента2_ — это R5.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**3\)** Поскольку на R4 пока нет мультикастового потока \(он его не запрашивал прежде\), он формирует сообщение PIM Join и отправляет его в сторону RP \(2.2.2.2\).

![\(\*, G\)](http://img-fotki.yandex.ru/get/9820/83739833.37/0_da314_6a1e3e1d_XXL.png)

PIM Join отправляется мультикастом на адрес 224.0.0.13. «В сторону RP» означает через интерфейс, который указан в таблице маршрутизации, как outbound для того адреса, который указан внутри пакета. В нашем случае это 2.2.2.2 — адрес RP. Такой Join обозначается ещё как **Join \(\*,G\)** и говорит: «Не важно, кто источник, мне нужен трафик группы 224.2.2.4».  
То есть каждый маршрутизатор на пути должен обработать такой Join и при необходимости отправить новый Join в сторону RP. \(Важно понимать, что если на маршрутизаторе уже есть эта группа, он не будет отправлять выше Join — он просто добавит интерфейс, с которого пришёл Join, в OIL и начнёт передавать трафик\).  
В нашем случае Join ушёл в FE0/1:

![show ip route](http://img-fotki.yandex.ru/get/9813/83739833.37/0_da315_d385f9e7_XL.png)

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**4\)** R2, получив Join, формирует запись \(\*, G\) и добавляет интерфейс FE0/0 в OIL. Но Join отсылать уже некуда — он сам уже RP, а про источник пока ничего не известно.

![](http://img-fotki.yandex.ru/get/9813/83739833.37/0_da316_a5b98bea_XL.png)  
Таким образом RP узнаёт о том, где находятся клиенты.

Если _Клиент 2_ тоже захочет получать мультикастовый трафик для той же группы, R5 отправит PIM Join в FE0/1, потому что за ним находится RP, R3, получив его, формирует новый PIM Join и отправляет в FE1/1 — туда, где находится RP.  
То есть Join путешествует так узел за узлом, пока не доберётся до RP или до другого маршрутизатора, где уже есть клиенты этой группы.

![](http://img-fotki.yandex.ru/get/6710/83739833.37/0_da317_d383a226_XL.png)

Итак, R2 — наш RP — сейчас знает о том, что за FE0/0 и FE1/0 у него есть получатели для группы 224.2.2.4.  
Причём неважно, сколько их там — по одному за каждым интерфейсом или по сто — поток трафика всё равно будет один на интерфейс.

Если изобразить графически то, что мы получили, то это будет выглядеть так:

![RP Tree](http://img-fotki.yandex.ru/get/9806/83739833.37/0_da318_4bb77bc6_XXL.png)

Отдалённо напоминает дерево, не так ли? Поэтому оно так и называется — **RPT — Rendezvous Point Tree**. Это дерево с корнем в RP, а ветви которого простираются до клиентов.  
Более общий термин, как мы упоминали выше, — **MDT — Multicast Distribution Tree** — дерево, вдоль которого распространяется мультикастовый поток. Позже вы увидите разницу между MDT и RPT.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**5\)** Теперь врубаем сервер. Как мы уже выше обсуждали, он не волнуется о PIM, RP, IGMP — он просто вещает. А R1 получает этот поток. Его задача — доставить мультикаст до RP.  
В PIM есть специальный тип сообщений — **Register**. Он нужен для того, чтобы зарегистрировать источник мультикаста на RP.  
Итак, R1 получает мультикастовый поток группы 224.2.2.4:

![show interface summary](http://img-fotki.yandex.ru/get/6729/83739833.38/0_da31b_832942ad_XXL.png)

![\(S, G\)](http://img-fotki.yandex.ru/get/6710/83739833.38/0_da31c_84967d3b_XL.png)

R1 является **FHR \(First Hop Router\)** — первый маршрутизатор на пути мультикастового трафика или ближайший к источнику.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**6\)** Далее он инкапсулирует каждый полученный от источника мультикастовый пакет в юникастовый PIM Register и отправляет его прямиком на RP.

![PIM Register](http://img-fotki.yandex.ru/get/9763/83739833.38/0_da31d_44c384a1_XXL.png)

Обратите внимание на стек протоколов. Поверх юникастового IP и заголовка PIM идёт изначальный мультикастовый IP, UDP и данные.  
Теперь, в отличие от всех других, пока известных нам сообщений PIM, в адресе получателя указан 2.2.2.2, а не мультикастовый адрес.

Такой пакет доставляется до RP по стандартным правилам юникастовой маршрутизации и несёт в себе изначальный мультикастовый пакет, то есть это… это же туннелирование!

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**7\)** RP получает PIM Register, распаковывает его и обнаруживает под обёрткой трафик для группы 224.2.2.4.  
Информацию об этом он сразу заносит в свою таблицу мультикастовой маршрутизации:

![\(S, G\)](http://img-fotki.yandex.ru/get/6729/83739833.38/0_da31f_7e396715_XL.png)

Появилась запись \(S, G\) — \(172.16.0.5, 224.2.2.4\).  
Распакованные пакеты RP дальше отправляет в RPT в интерфейсы FE0/0 и FE1/0, по которому трафик доходит до клиентов.

В принципе, на этом можно было бы и остановиться. Всё работает — клиенты получают трафик. Но есть две проблемы:

1. Процессы инкапсуляции и декапсуляции — весьма затратные действия для маршрутизаторов. Кроме того, дополнительные заголовки увеличивают размер пакета, и он может просто не пролезть в MTU где-то на промежуточном узле \(вспоминаем все проблемы [туннелирования](http://www.opennet.ru/base/cisco/gre_fragment.txt.html)\).
2. Если вдруг где-то между источником и RP есть ещё получатели для группы, мультикастовому трафику придётся пройти один путь дважды.

Возьмём для примера вот такую топологию:

![PIM Register Encapsulation](http://img-fotki.yandex.ru/get/6727/83739833.37/0_da319_ddb77e2d_XXL.png)

Трафик в сообщениях Register сначала дойдёт до RP по линии R1-R42-R2, затем чистый мультикаст вернётся по линии R2-R42. Таким образом на линии R42-R2 будет ходить две копии одного трафика, пусть и в противоположных направлениях.

![Инкапсуляция PIM Register](http://img-fotki.yandex.ru/get/9169/83739833.38/0_da31a_4f499e56_XXL.png)

Поэтому лучше от источника до RP тоже передавать чистый мультикаст, а для этого нужно построить дерево — **Source Tree**.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**8\)** Поэтому RP отправляет на R1 сообщение PIM Join. Но теперь уже в нём указывается для группы адрес не RP, а источника, изученный из сообщения Register. Такое сообщение называется **Join \(S, G\) — Source Specific Join**.

![Source-Specific Join](http://img-fotki.yandex.ru/get/6727/83739833.38/0_da320_f23cd1ed_XXL.png)  
Цель у него точно такая же, как у PIM Join \(_, G\) — построить дерево, только на этот раз от источника до RP.  
Join \(S, G\) распространяется также узел за узлом, как обычный Join \(_, G\). Только Join \(\*, G\) стремится к RP, а Join \(S, G\) к S — источнику. В качестве адрес получателя также служебный адрес 224.0.0.13 и TTL=1.

> Если существуют промежуточные узлы, например, R42, они также формируют запись \(S, G\) и список нисходящих интерфейсов для данной группы и пересылают Join дальше к источнику.

Путь, по которому прошёл Join от RP до источника, превращается в **Source Tree** — дерево от источника. Но более распространённое название — **SPT — Shortest Path Tree** — ведь трафик от источника до RP пойдёт по кратчайшему пути.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**9\)** R1 получив Join \(S, G\), добавляет интерфейс FE1/0, откуда пакет пришёл, в список нисходящих интерфейсов OIL и начинает туда вещать чистый мультикастовый трафик, незамутнённый инкапсуляцией. Запись \(S, G\) на R1 уже была, как только он получит первый мультикастовый пакет от Сервера-источник.

![\(S, G\)](http://img-fotki.yandex.ru/get/9820/83739833.38/0_da31e_c1e554e6_XL.png)

По построенному Source Tree мультикаст передаётся RP \(и всем промежуточным клиентам, если они есть, например, R42\).

Но надо иметь ввиду, что сообщения Register передавались всё это время и передаются до сих пор. То есть фактически R1 отправляет две копии трафика сейчас: один — чистый мультикаст по SPT, другой — инкапсулированный в юникастовый Register.

![PIM Register и UDP одновременно](http://img-fotki.yandex.ru/get/9497/83739833.38/0_da321_87614b7d_XXL.png)

> Сначала R1 отправляет мультикаст в Register — **пакет 231**. Потом R2 \(RP\) хочет подключиться к дереву, отправляет Join — **пакет 232**. R1 ещё какое-то время, пока обрабатывает запрос от R2, отправляет мультикаст в Register \(**пакеты с 233 по 238**\). Далее, когда нисходящий интерфейс добавлен в OIL на R1, он начинает передавать чистый мультикаст — **пакеты 239 и 242**, но пока не прекращает и Register — **пакеты 241 и 243**. А **пакет 240** — это R2 не выдержал и ещё раз попросил построить дерево.

![](http://img-fotki.yandex.ru/get/6729/83739833.39/0_da9ff_770594e6_M.png)

**10\)** Итак, незамутнённый мультикаст достигает RP. Она понимает, что это тот же самый трафик, который приходит в Register, потому что одинаковый адрес группы, одинаковый адрес источника и с одного интерфейса. Чтобы не получать две копии, он отправляет на R1 юникастовый **PIM Register-Stop**.

![PIM Register-Stop](http://img-fotki.yandex.ru/get/9763/83739833.38/0_da322_749fa33e_XXXL.png)

Register-Stop не означает, что R2 отказывается от трафика или не признаёт больше этот источник, это говорит лишь о том, что надо прекратить посылать **инкапсулированный** трафик.

Далее идёт ожесточённая борьба — R1 продолжает передавать накопившийся в буфере трафик, пока обрабатывает Register-Stop, и обычным мультикастом и внутри сообщений Register:

![Мультикастовые соревнования](http://img-fotki.yandex.ru/get/9795/83739833.39/0_de2aa_fa550ead_XXXL.png)

Но, рано или поздно R1 начинает вещать только чистый мультикастовый трафик.

![Multicast UDP](http://img-fotki.yandex.ru/get/9748/83739833.38/0_da324_a862421b_XXXL.png)

> При подготовке у меня возник, как мне казалось, закономерный вопрос: ну и к чему все эти туннелирования, PIM Register? Почему бы не поступать с мультикастовым трафиком, как с PIM Join — отправлять хоп за хопом с TTL=1 в сторону RP — рано или поздно ведь дойдёт? Так бы и дерево построилось бы заодно без лишних телодвижений.  
> Тут возникает несколько нюансов.  
> Во-первых, нарушается главный принцип PIM SM — трафик посылать только туда, откуда он был запрошен. **Нет Join — Нет дерева**!  
> Во-вторых, если клиентов для данной группы нет, FHR не узнает об этом и будет продолжать слать трафик по «своему дереву». К чему такое бездумное использование полосы пропускания? В мире связи такой протокол просто не выжил бы, как не выжил PIM DM или DVMRP.

Таким образом мы имеем одно большое дерево MDT для группы 224.2.2.4 от _Cервера-источника_ до _Клиента 1_ и _Клиента 2_. И это MDT составлено из двух кусков, которые строились независимо друг от друга: **Source Tree** от источника до RP и **RPT** от RP до клиентов. Вот оно отличие MDT от RPT и SPT. MDT — это довольно общий термин, означающий дерево передачи мультикаста вообще, в то время, как RPT/SPT — это его очень конкретный вид.

А что делать, если сервер уже вещает, а клиентов всё нет и нет? Мультикаст так и будет засорять участок между отправителем и RP?  
Нет, в этом случае также поможет PIM Register-Stop. Если на RP начали приходить сообщения Register для какой-то группы, а для неё ещё нет получателей, то RP не заинтересован в получении этого трафика, поэтому, **не отправляя** PIM Join \(S, G\), RP сразу посылает Register-Stop на R1.  
R1, получив Register-Stop и видя, что дерева для этой группы пока нет \(нет клиентов\), начинает отбрасывать мультикастовый трафик от сервера.  
То есть сам сервер по этому поводу совершенно не беспокоится и продолжает посылать поток, но, дойдя до интерфейса маршрутизатора, поток будет отброшен.  
При этом RP продолжает хранить запись \(S, G\). То есть трафик он не получает, но где находится источник для группы знает. Если в группе появляются получатели, RP узнаёт о них и посылает на источник Join \(S, G\), который строит дерево.

Кроме того, каждые 3 минуты R1 будет пытаться повторно зарегистрировать источник на RP, то есть отправлять пакеты Register. Это нужно для того, чтобы уведомить RP о том, что этот источник ещё живой.

> У особо пытливых читателей обязан возникнуть вопрос — а как быть с RPF? Этот механизм ведь проверяет адрес отправителя мультикастового пакета и, если трафик идёт не с правильного интерфейса, он будет отброшен. При этом RP и источник могут находиться за разными интерфейсам. Вот и в нашем примере для R3 RP — за FE1/1, а источник — за FE1/0.  
> Ответ предсказуем — в таком случае проверяется не адрес источника, а RP. То есть трафик должен придти с интерфейса в сторону RP.  
> Но, как вы увидите далее, это тоже не нерушимое правило.

Важно понимать, что RP — это не универсальный магнит — для каждой группы может бытья своя RP. То есть в сети их может быть и две, и три, и сто — одна RP отвечает за один набор групп, другая — за другой. Более того, есть такое понятие, как [**Anycast RP**](http://lookmeup.linkmeup.ru/#term314) и тогда разные RP могут обслуживать одну и ту же группу.
