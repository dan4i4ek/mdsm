# Control Plane

**Плоскость управления**

Всему голова. Она **заранее** заполняет таблицы, по которым затем будет передаваться трафик.  
Здесь работают протоколы со сложными алгоритмами, которые дорого или невозможно выполнить аппаратно.

Например, [алгоритм Дейкстры](http://lookmeup.linkmeup.ru/#term256) реализовать на чипе можно, но сложно. Так же сложно сделать выбор лучшего маршрута BGP или определение FEC и рассылку меток. Кроме того, для всего этого пришлось бы делать отдельный чип или часть чипа, которая практически не может быть переиспользована.  
В такой ситуации лучше пожертвовать сабсекундной сходимостью в пользу удобства и цены.

Поэтому ПО запускается на CPU общего назначения.  
Получается медленно, но гибко — вся логика программируема. И на самом деле скорость на Control Plane не важна. Однажды вычисленный маршрут инсталлируется в FIB, а дальше всё на скорости линии.  
Вопрос скорости Control Plane возникает при обрывах, флуктуациях на сети, но он сравнительно успешно решается механизмами TE HSB, TE FRR, IP FRR, VPN FRR, когда запасные пути готовятся заранее на том же Control Plane.

> Примеры:
>
> 1. Запустили сеть с IGP. Нужно сформировать Hello, согласовать параметры сессий, обменяться базами данных, просчитать кратчайшие маршруты, инсталлировать их в Таблицу Маршрутизации, поддерживать контакт через периодические Keepalive.
> 2. Пришёл [BGP Update](http://linkmeup.ru/blog/65.html). Control Plane добавляет новые маршруты в таблицу BGP, выбирает лучший, инсталлирует его в Таблицу Маршрутизации, при необходимости пересылает Update дальше.
> 3. Администратор включил [LDP](http://linkmeup.ru/blog/154.html). Для каждого префикса создаётся [FEC](http://lookmeup.linkmeup.ru/#term477), назначается метка, помещается в таблицу меток, анонсы уходят всем LDP-соседям.
> 4. Собрали два коммутатора в стек. Выбрать главный, проиндексировать интерфейсы, актуализировать таблицы пересылок — задача Control Plane.

Работа и реализация Control Plane универсальна: ЦПУ + оперативная память: работает одинаково хоть на стоечных маршрутизаторах, хоть на виртуальных сетевых устройствах.

Эта система — не мысленный эксперимент, не различные функции одной программы, это действительно физически разделённые тракты, которые взаимодействуют друг с другом.  
Началось всё с разнесения плоскостей на разные платы. Далее появились стекируемые устройства, где одно выполняло интеллектуальные операции, а другое было лишь интерфейсным придатком.  
Вчерашний день — это системы вроде Cisco Nexus 5000 Switch + Nexus 2000 Fabric Extender, где 2000 выступает в роли выносной интерфейсной платы для 5000.  
Где-то в параллельной Вселенной тихо живёт SDN разлива 1.0 — с Openflow-like механизмами, где Control Plane вынесли на внешние контроллеры, а таблицы пересылок заливаются в совершенно глупые коммутаторы.  
Наша реальность и ближайшее будущее — это наложенные сети \(Overlay\), настраиваемые SDN-контроллерами, где сервисы абстрагированы от физической топологии на более высоком уровне иерархии.  
И несмотря на то, что с каждой статьёй мы всё глубже погружаемся в детали, мы учимся мыслить свободно и глобально.

Разделение на Control и Forwarding Plane позволило отвязать передачу данных от работы протоколов и построения сети, а это повлекло значительное повышение масштабируемости и отказоустойчивости.  
Так один модуль плоскости управления может поддерживать несколько интерфейсных модулей.  
В случае сбоя на плоскости управления механизмы [GR, NSR](https://www.cisco.com/c/en/us/products/collateral/ios-nx-os-software/high-availability/solution_overview_c22-487228.html), [GRES](https://www.juniper.net/documentation/en_US/junos/topics/concept/gres-overview.html) и [ISSU](https://www.juniper.net/documentation/en_US/junos/topics/concept/issu-on-qfx5100-overview.html) помогают плоскости пересылки продолжать работать будто ничего и не было.

