# Локальные пакеты

Бо́льшая часть локальных пакетов обрабатываются на ЦПУ.  
Напомню, что локальные — это те, которые были созданы на данном узле, которые предназначены именно ему \(юникастовые\), которые предназначены всем/многим \(броадкастовые или мультикастовые\) или которые намеренно требуют обработки на ЦПУ \(TTL Expired, Router Alert\).

**Входящие**  
Вплоть до FE с ними происходит всё то же самое, что и с транзитными. Далее чип коммутации, обратившись в CAM, видит, что DMAC — это MAC-адрес локального устройства, заглядывает в EtherType. Если это какой-нибудь BPDU или ISIS PDU, то пакет сразу передаётся нужному протоколу.  
Если IP — передаёт его модулю IP, который, заглядывая в TCAM, видит, что и [DIP](http://lookmeup.linkmeup.ru/#term53) тоже локальный — значит нужно посмотреть в поле Protocol заголовка IPv4 \(или Next Header IPv6\).  
Определяется протокол, принимается решение о том, какому модулю дальше передать пакет — BFD, OSPF, TCP, UDP итд. И так пакет разворачивается до конца, пока не будет найдено приложение назначения.  
Когда Ingress FE с этим справился, содержимое пакета передаётся на CPU через специальный канал связи.  
На этом шаге достаточно интеллектуальные устройства применяют политику по ограничению скорости протокольных пакетов, передаваемых на ЦПУ, чтобы одними только telnet'ами не заDoSить процессор.

![](../../.gitbook/assets/image-173.png)

Если данный пакет принёс информацию об изменении топологии \(например, новый OSPF, LSA\), Control Plane долежен обновить Soft Tables \(RAM\), а затем изменения спускаются в Hard Tables \(CAM/TCAM+RAM\).  
Если пакет требует ответа, то устройство должно его сформировать и отправить назад изначальному источнику \(например, TCP Ack на пришедший BGP Update\) или передать куда-то дальше \(например, OSPF LSA или RSVP Resv\).

**Исходящие** протокольные пакеты формируются на ЦПУ — он заполняет все поля всех заголовков на основе Soft Tables и далее, в зависимости от реализации, спускает его на Ingress или Egress FE.

> Из-за того, что пакет сформирован на процессоре, зачастую он не попадает под **интерфейсные** политики. Архитектурно многие операции, выполняющиеся на FE, требуют того, чтобы FE прозизводил Lookup и формировал заголовки.  
> Отсюда могут быть любопытные и неочевидные следствия, например, их не получится отловить ACL, вы можете не увидеть их в зазеркалированном трафике, они не будут учитываться при ограничении скорости. Но это не точно, зависит от вендора и оборудования.  
> Однако политики, работающие с очередями на CPU их, конечно, увидят.

Есть некоторые протоколы Control Plane, которые всё-таки обрабатываются в железе. Ярким примером может служить BFD. Его таймеры выкручиваются вплоть до 1 мс. CPU, как мы помним, штука гибкая, но неповоротливая, и пока BFD-пакет пройдёт по всему тракту и развернётся до заголовка BFD, пока до процессора дойдёт прерывание, пока тот на него переключится, прочитает пакет, сгенерирует новый, вышлет его, пройдут десятки и сотни миллисекунд — глядь, а BFD-то уже развалился.

Поэтому пакеты BFD в большинстве случаев разбираются на чипе, на нём же и готовится ответ. И только сама сессия устанавливается через CPU.

Большие в этом вопросе пошли ещё дальше, перенеся на железо наиболее рутинные операции.

Так, например, Juniper ввёл [**PPM — Periodic Packet Management**](https://www.juniper.net/documentation/en_US/junos/topics/concept/routing-distributed-periodic-packet-management-ex-series.html), который разделяет функции Control Plane некоторых протоколов между управляющим модулем и интерфейсным:

* Bidirectional Forwarding Detection \(BFD\)
* Connectivity Fault Management \(CFM\)
* Link Aggregation Control Protocol \(LACP\)
* Link Fault Management \(LFM\)
* Multiprotocol Label Switching \(MPLS\)
* Real-time Performance Monitoring \(RPM\)
* Spanning Tree Protocol \(STP\)
* Synchronous Ethernet \(SYNCE\)
* Virtual Router Redundancy Protocol \(VRRP\)

История выше отсылает нас к длинным пингам. Иногда инженер проверяет RTT своей сети путём пинга с одного маршрутизатора на другой. Видит вариацию в десятки и сотни мс и, начиная переживать, открывает запросы вендору. Пугаться тут нечего. Обычно ICMP обрабатывается на CPU. И именно занятостью процессора определяется время ответа. При этом корреляция с реальным RTT сети практически нулевая, потому что транзитный трафик на CPU не обрабатывается.  
Некоторые современные сетевые устройства могут обрабатывать ICMP-запросы и формировать ICMP-ответы на чипе \(NP, ASIC, FPGA\), минуя долгий путь до CPU. И вот в этом случае циферки в ping будут адекватны реальности.  
Кроме того, есть технологии мониторинга качества сети \([OAM](http://blog.sbolshakov.ru/12-ethernet-oam/)\), работающие аппаратно, например [CFM](https://www.cisco.com/c/en/us/td/docs/net_mgmt/prime/network/3-9/reference/guide/PrimeNetwork39_RefGuide/cfm_chapter.pdf).

