Знаете ли вы, что количество серверов у братьев наших старших давно перевалило за <a href="https://translate.google.com/translate?hl=en&sl=auto&tl=en&u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FNKxDZsqeEzuj7NYV_3xaFA" target="_blank">миллион</a>.
Они не работают каждый сам по себе, и они не подключены по <a href="https://en.wikipedia.org/wiki/Wireless_data_center" target="_blank">Wi-Fi</a>, а это значит, что их нужно объединять в кабельную сеть. И для этого строят гигантскую сложную дорогую инфраструктуру. Сетевой архитектор Яндекса Дмитрий Афанасьев <a href="https://www.youtube.com/watch?v=U86Xjx1rcHY&t=1051s" target="_blank">рассказывал</a> на прошедшей в 2019 конференции NextHop о подходах и возникающих в самых неожиданных местах нюансах.

Если сегодня вы решите построить что-то очень большое, вроде Яндекса или Гугла, то выбор топологии сети предопределён - это будет сеть Клоза.
В этой статье я вместе с вами разберусь в эволюции подходов к строительству датацентров и причинах появления тех или иных решений. 

Сеть в большой степени следует за требованиями сервисов, лишь иногда диктуя, как было бы правильно сделать.
Так, приложения затребовали широкую полосу и низкие задержки - и сети пришлось стать очень регулярной и плотной.
Но при таком дизайне плоской сетью с Ethernet-коммутацией уже не обойдёшься, и в современном мире приложениям приходится учится жить в мире IP-маршрутизации.

<a href="https://fs.linkmeup.ru/images/adsm/2/kdpv.jpg" target="_blank"><img src="https://fs.linkmeup.ru/images/adsm/2/kdpv_small.jpg" width="600"></a>

Спойлер: • Clos ••• L3 ••• ECMP ••• BGP ••• Overlay ••• Single-chip •

<h1>Содержание</h1>
<ul>
    <li><b><a href="#3_TIER">Трёхуровневая иерархическая сеть</a></b></li>
    <li><b><a href="#NSVSEW">Север-Юг против Запада-Востока</a></b></li>
    <li><b><a href="#CLOS">Сети Клоза</a></b>
    <ul>
        <li><b><a href="#BLOCKING">Блокируемые и неблокируемые сети и переподписка</a></b></li>
        <li><b><a href="#MULTISTAGE">Многоуровневая сеть Клоза</a></b></li>
        <li><b><a href="#GLOSSARY">3-Tier, Fat Tree, Clos, Folded Clos, Leaf-Spine?! Аааа!</a></b></li>
        <li><b><a href="#ROUTING">Маршрутизация</a></b></li>
        <li><b><a href="#TUNNELS">Вам IP или туннелировать?</a></b></li>
        <li><b><a href="#DRAWBACKS">Минусы сети Клоза</a></b></li>
    </ul>
    </li>
    <li><b><a href="#OTHERTOPOS">Альтернативные топологии</a></b></li>
    <li><b><a href="#DEVICES">Выбор оборудования</a></b></li>
    <li><b><a href="#THEEND">Заключение</a></b></li>
    <li><b><a href="#LINKS">Полезные ссылки</a></b></li>
</ul>
<hr>

<cut>
Если заглянуть в далёкое прошлое датацентры жили примерно в таких условиях:
<ul>
    <li>Преобладающий North-South трафик от клиента к серверу и обратно.</li>
    <li>Плоская Ethernet-сеть (L2) в силу требования мобильности виртуальных машин без смены IP-адреса и высокой цены на L3-оборудование.</li>
    <li>Любое сетевое оборудование, обладающее широкой функциональностью, стоило действительно немалых денег.</li>
</ul>

Поэтому традиционная сеть ДЦ была такой:

<a name="3_TIER"></a>
<h1>Трёхуровневая иерархическая сеть - 3-Tier</h1>
<img src="https://fs.linkmeup.ru/images/adsm/2/fat_tree.png" width="600">

Это классическая трёхуровневая архитектура сети, известная нам с <a href="https://linkmeup.ru/blog/11.html" target="_blank">СДСМ0</a>: Access, Aggreggation/Distribution/Core.
Так были построены сети операторов связи и предприятий, а впоследствие её же перенесли в датацентры, потому что только так и умели, да и оборудования другого не было.

Чем дальше в ядро, тем толще линки.
- 100 Мб/с к машинам, 1 Гб/с от коммутаторов доступа к коммутаторам агрегации. 
- 10 Гб/с (а в то время, возможно, LAG из нескольких гигабитных линков) к ядру сети.
- Итд.
Потому что весь трафик от серверов сливался в аплинки.

Наверху иерархии стояла пара <a href="https://comeroutewithme.com/2015/02/28/why-is-clos-spineleaf-the-thing-to-do-in-the-data-center/">God Boxes</a>, концентрирующих на себе все сервисы и всю маршрутизацию. На них инженеры молились и старались не дышать. 
Это были модульные шкафы, занимающие полстойки, а то и стойку, а то и кластера из нескольких стоек. 

С этой топологией была <i>пара</i> проблем.
Если взять ситуацию, как на рисунке выше, то отказ любого линка сразу же приводит к выходу из строя части сети.
Фактически же строили сети всё-таки с резервированием, и на каждом уровне обычно стояло по паре устройств:
<img src="https://fs.linkmeup.ru/images/adsm/2/fat_tree_redundancy.png" width="600">
Да, это требовало богомерзкого STP, но это позволяло обеспечить хоть какую-то стабильность.

Здесь <b>выпадение</b> линка или устройства приводило к двукратной деградации пропускной способности и, очевидно, работе без резерва.

Кроме того, все сервисы при таком подходе сосредотачивались как раз на этих верхних устройствах
<b>Вывести их из эксплуатации</b>, например, для обновления, означало также потерю половины пропускной способности сети, а то и вовсе полную недоступность сервисов из-за того что второе устройство настроено неправильно.
<blockquote>
    О, сколько раз на моей памяти инженер со спокойной душой выводил устройство из эксплуатации со словами "второе же есть" и ронял сеть, потому что на втором кто-то в своё время превентивно погасил интерфейс или положил BGP-сессию.
</blockquote>

<b>Расширение пропускной способности</b> - это отдельная болезненная история:
<ul>
    <li>Расширение LAG'ов,</li>
    <li>Закупка линейных карт.</li>
    <li>А если карты ставить больше некуда, то апгрейд железа, на ещё более мощные и большие ящики.</li>
</ul>

<b>Итого:</b>
<ol>
    <li>Классическая трёхуровневая сеть не может обеспечить лёгкое расширение пропускной способности.</li>
    <li>Она довольно плохо масштабируется: добавление нового модуля в ДЦ потребует снова расширения линков.</li>
    <li>Она не может обеспечить высокую степень резервирования - в L2-топологии использование избыточных линков затруднительно из-за "особенностей" MSTP. Соответственно часть линков просто простаивает.  А при выпадении узла есть риск потерять не только пропускную способность, но и всю сеть на некоторое время.</li>
    <li>В общем, эксплуатировать и поддерживать классическую сеть проблематично.</li>
</ol>

<blockquote>
    Любопытно, что L2-топология, обычно используемая в датацентрах прежде, настолько избаловала разработчиков приложений и системных администраторов, что они долгое время не представляли своей жизни без L2. 
    Здесь и кроются истоки TRILL, SPB, FabricPath и прочих технологий, так желающих облегчить жить Ops'ам.
    И только в последние годы прослеживается устойчивая тенденция к L3 до стойки в датацентрах.

    Однако было бы лукавством сказать, что L2 отживает своё. Зачастую он просто переместился на уровень абстракции выше - а именно в <a href="https://linkmeup.ru/blog/449.html#OVERLAY" target="_blank">оверлей</a>.
    Тот же VXLAN благополучно растягивает L2 домен до треска. EVPN тоже умеет его сигнализировать.
    Однако общая тенденция такова, что на физической сети, в <a href="https://linkmeup.ru/blog/449.html#UNDERLAY" target="_">андерлее</a>, остаётся исключительно L3.
</blockquote>
<hr>

<a name="NSVSEW"></a>
<h1>Север-Юг против Запада-Востока</h1>
Однако всё вышеперечисленное в некотором смысле мелочи, которые можно было бы решить. Один переход на L3 снял бы бо́льшую часть сложностей.
Проблема в том, что изменился мир.
Внезапно появились Big Data, Map Reduce, ML, гигантские базы данных, аналитика, контекстная реклама. А теперь и AI из пауэр-поинтов на нас поглядывает. 

Исторически запрос пользователя полностью обрабатывался в рамках одного хоста. Был запрос от клиента, который пришёл сверху, и ответ, уходящий обратно наверх.
В такой сети преобладает вертикальное (North-South) направление. И иерархическая сеть такой спрос вполне удовлетворяла. 
<img src="https://fs.linkmeup.ru/images/adsm/2/north_south.png" width="600">

Теперь в ДЦ преобладает <b>горизонтальный (East-West)</b> трафик между серверами. 
И вот как это получается:
<ol>
    <li>Клиент запросил веб-страничку,</li>
    <li>Его запрос пришёл на балансировщик трафика,</li>
    <li>Балансировщик перенаправил его на один из фронтендов, </li>
    <li>Тот послал запрос на один бэкенд сервер, чтобы получить текстовый контент,</li>
    <li>На другой сервер, чтобы получить данные о пользователе: пол, возраст, предпочтения, последний поиск,</li>
    <li>На третий - проанализировать данные по локации,</li>
    <li>Второй и третий послали новые данные на четвёртый, чтобы пополнить информацию о пользователе в БД, чтобы ещё лучше знать какие отмычки для лифтовых дверей предпочитает среднестатистический житель Ленинского района города Новосибирска, пользующийся Android 9.1,</li>
    <li>На пятый - сформировать контекстную рекламу,</li>
    <li>Потом все разом начали отвечать фронтенду,</li>
    <li>Фронтенд отдал всю страницу обратно</li>
</ol>

<img src="https://fs.linkmeup.ru/images/adsm/2/west_east.png" width="600">

То есть бо́льшая часть событий произошла внутри сети сервис-провайдера.
При этом запрос-ответ внутри ДЦ может многократно превышать по размеру запрос-ответ пользователю. Добавим к этому репликации БД, внутренние хранилища, логи и всё становится понятно (кивните, если согласны). 

Вот такой график <a href="https://www.youtube.com/watch?v=mLEawo6OzFM" target="_blank">Facebook показывает</a> на конференциях, иллюстрируя тенденцию к стремительному росту intra-dc трафика:
<img src="https://fs.linkmeup.ru/images/adsm/2/facebook_traffic.png" width="400">

Традиционные сети органически не приспособлены к этому из-за своего акцента на North-South.
L2-топологиям из-за низкой утилизации полосы и склонности к штормам тоже отказано (TRILL бы, возможно, спас ситуацию, но где он?).
Ну, и умное железо если и не стало резко дешевле, то только из-за всё возрастающей производительности, доходящей сегодня уже до 12,8 Тб/с на одночиповую коробку размером 4 юнита.
А так уже любая вафельница умеет в L3+BGP.
<hr>

<a name="CLOS"></a>
<h1>Сети Клоза</h1>
Современный подход к строительству ДЦ, которые используют гипер-скейлеры и <a href="https://www.instagram.com/p/ByJoMiRB4bW/" target="_blank">клауд-титаны</a>, вроде Яндекса, Фейсбука и Гугла - развитие идей крутого парня из Bell Laboratories Чарльза Клоза (который на самом-то деле Шарль Кло, ибо француз).

<img src="https://fs.linkmeup.ru/images/adsm/2/charles_clos.jpg" width="600">

Он в 1953 году, решая проблему проключения звонков через телефонную станцию, предложил неблокируемую сеть, в которой любые входы могут установить соединение с любыми выходами в любой момент времени.

Прежде для решения этой задачи использовался так называемый <b>Crossbar</b>, соединявший каждый вход с каждым выходом.
<img src="https://fs.linkmeup.ru/images/adsm/2/cross_bar.png" width="400">
<i>Изображение с сайта <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/" target="_blank">packetpushers.net</a></i>

Учитывая, что большая их часть простаивала, решение, и без того сложное в реализации (n^2 соединений), было ещё и неоптимальным.

Он предложил разделить входы и выходы дополнительным уровнем коммутации, который позволит простраивать соединения между входами и выходами по запросу. Это и назвали <b>сетью Клоза</b>.
<img src="https://fs.linkmeup.ru/images/adsm/2/clos_network_1.png" width="600">
<i>Изображение с сайта <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/" target="_blank">packetpushers.net</a></i>

Сложность такой сети значительно ниже, чем у Crossbar, поскольку имеет гораздо меньшее количество точек коммутации: 6n^(3/2)-3n.

Вот таблица необходимого числа коммутаций:
<img src="https://fs.linkmeup.ru//images/adsm/2/crosspoint_complexity.png" width="600">

Уже начиная с 36 конечных точек, сложность топологии Клоза меньше, чем связей "каждый с каждым".

При этом оригинальная сеть Клоза подразумевает отсутствие блокировки, то есть любые два входа всегда могут быть скоммутированы друг с другом. Выделяют строго неблокируемые или неблокируемые при перекоммутациях - для нас это сейчас неважно. Фактически же более рационально использовать сети с низкой вероятностью блокировки.


В чистом виде топология Клоза описывала сети с коммутацией каналов.
Но в 90-е годы с развитием пакетной коммутации эта идея обрела альтер эго.
В модульном сетевом оборудовании стало уже сложно соединять все <a href="https://linkmeup.ru/blog/312.html#SERDES" target="_blank">SerDes'ы</a> одного чипа со всеми SerDes'ами другого, поэтому появился дополнительный уровень, в общем скрытый от пользователей, <b><a href="https://linkmeup.ru/blog/312.html#FABRIC">фабрики коммутации</a></b>, единственной задачей которых было доставить пакеты от входного порта в выходной, убрав при этом необходимость в полносвязной топологии.
Это не что иное, как сеть Клоза.

Ну и последняя на сегодняшний день инкарнация - это <b>датацентровые сети</b>, которые суть - те же фабрики коммутации.

Типичный вид трёхуровневой сети Клоза:
<img src="https://fs.linkmeup.ru/images/adsm/2/clos_network_2.png" width="600">

Более привычный вид Leaf-Spine - ещё он называется Folded Clos - потому что словно действительно сложен пополам.
<img src="https://fs.linkmeup.ru/images/adsm/2/clos_network_3.png" width="600">

Другое название такой сети - <b>Fat Tree</b>.

Вместо весьма интеллектуального, а соответственно и склонного к ошибкам и багам уровня агрегации появляется примитивный уровень коммутации - Spine, задача которого - очень быстро переложить пакет с одного Leaf на другой.

К Leaf'ам подключаются машины. Сами Leaf'ы подключаются к каждому Spine'у. А Spine'ы соответственно ко всем Leaf'ам.
Таким образом между любой парой машин будет существовать большое количество равноценных путей (по количеству спайнов) с всегда одинаковым числом хопов - 3 для сети, изображённой выше.

Выход же во внешний мир или в другие ДЦ обычно реализуется через отдельные коробки, которые с точки зрения фабрики выглядят как Leaf-коммутаторы, однако гораздо более функциональные. Называются они Edge-Leaf. 

<img src="https://fs.linkmeup.ru/images/adsm/2/clos_network_4.png" width="600">

<blockquote>
    Впрочем привычная операторам реализация границы датацентра всё же имеет право на жизнь. В этом случае функции Edge выполняют спайны.
</blockquote>

В такой топологии интеллект отчасти перемещается на Leaf'ы, отчасти на Edge-Leaf'ы

В чём плюсы такой сети?
<ol>
    <li>Во-первых, в отсутствии неиспользуемых линков (при учёте, что мы отказываемся от L2). <a href="https://linkmeup.ru/blog/482.html" target="_blank">ECMP</a> - это воздух для сетей Клоза.</li>
    <li>Во-вторых, конечно, в широких горизонтальных каналах для East-West-трафика.</li>
    <li>В-третьих, выпадение одного устройства или линка не влечёт фатальных последствий:
    Если это был ToR - то пострадает только одна стойка.
    Если Spine - просядет пропускная способность, но не на 50%, как это было бы прежде, а лишь на 1/n, где n - число спайнов. </li>
    <li>В-четвёртых, простота вывода спайнов из эксплуатации. Благодаря небольшой деградации и отсутствию интеллекта на этом узле, проводить работы на них не так страшно, как на God-Box'ах</li>
    <li>В-пятых, масштабируемость. 
        Новые лифы можно безболезненно добавлять пока не кончатся порты на спайнах.
        Добавлением спайна можно расширить аплинки лифов. 
        Добавлением эджей - полосу пропускания наружу.

    Не нужно расширять LAG'и и вставлять новые платы. А если вдруг портов потенциально не хватает или хочется объединять модули одного ДЦ в суперфабрику, то можно надстроить ещё один уровень спайнов (или не один) - хотя тут есть <a href="#DRAWBACKS">нюансы</a>.
    <blockquote>
        Такой метод масштабирования называется <b>горизонтальным</b> или <b>Scale out</b>. Добавляется просто ещё одно типовое устройство, практически линейно увеличивая пропускную способность. Нынче это очень модно. Теперь всё масштабируют горизонтально.
        Классический подход - <b>вертикальное</b> масштабирование <b>Scale up</b> - тут добавляют мощностей в текущие устройства, чтобы расширить их возможности. В маршрутизаторы добавляют платы, в сервера - оперативную память и процы.
    </blockquote>
    </li>
</ol>

Здесь может сложиться впечатление, что мы просто перенесли всю сложность с God-Box'ов на граничные маршрутизаторы. Что же, впечатление не обманывает: так оно и есть. Но L3 здесь позволяет, используя <a href="https://linkmeup.ru/blog/482.html" target="_blank">ECMP</a>, масштабировать внешнюю связность, просто увеличивая количество граничных коробок (см. выше Scale out). И также легко и безопасно выводить их из работы, как спайны (почти). 
<hr>

<a name="BLOCKING"></a>
<h2>Блокируемые и неблокируемые сети и переподписка</h2>
Как уже говорилось выше, топологии Клоза были придуманы для сетей с коммутацией каналов.
Поэтому свойство "строго неблокируемые" означало, что независимо от количества уже активных каналов, всегда будет возможность соединить свободный вход со свободным выходом.
Неблокируемые при перекоммутации - это топологии, в которых тоже всегда возможно найти свободный канал, но, возможно, придётся перекоммутировать существующий активный. Они требуют меньшее количество свитчей.
Так же существуют топологии с низкой вероятностью блокировки.

Если переложить эти термины на сеть с коммутацией пакетов, то неблокируемая сеть Клоза описывает в некотором смысле фабрику без переподписки, когда сумма аплинков не меньше суммы даунлинков. Например, схема с 30ю даунлинками по 10Гб/с и 3мя аплинками по 100Гб/с на Leaf - это фабрика без переподписки - и даже при одновременной передаче данных всеми подключенными хостами - всегда хватит аплинков.
Фактически это не всегда необходимо. Глядя на пиковую утилизацию аплинков, например, мы видим, что она достигает 80Гб/с, мы не ожидаем рост, и тогда можно делать два аплинка по 100Гб/с. Получим переподиску 2:3, аналогичную сети Клоза с низкой вероятностью блокировки.
Строить фабрику с переподпиской или без - вопрос на усмотрение сетевых архитекторов, но возьму на себя смелость сказать, что чаще строят с переподпиской, чтобы сэкономить порты на спайнах.
Касательно пропускной способности стоит ещё отметить ситуацию с кратковременными всплесками трафика (микробёрстами), рождёнными, например, <a href="http://bradhedlund.com/2011/05/01/tcp-incast-and-cloud-application-performance/" target="_blank">инкастом</a>. С ними не справится и фабрика без переподписки, поскольку мы упираемся обычно не в общую полосу аплинков, а в скорость порта, который не успевает пропустить трафик, начинает его буферизировать, очереди переполнятся и пакеты дропаются. Но это уже совсем другая история.


<a name="MULTISTAGE"></a>
<h2>Многоуровневая сеть Клоза</h2>
Рассмотренная выше сеть Клоза - трёхуровневая или 3-stage: Input Switch - Middle Switch - Output Switch. Если в ней одни и те же лифы выполняют роль одновременно и передатчиков и приёмников, её можно сложить пополам по линии Middle Switch и получается более привычная нам двухуровневая сеть Leaf-Spine.

Пятиуровневая сеть Клоза будет выглядеть так: Input Switch - Middle Switch 1 - Middle Switch 2 - Middle Switch 3 - Output Switch. Вот она повёрнутая на 90 градусов:
<img src="https://fs.linkmeup.ru/images/adsm/2/5_stage_clos.png" width="700">

Если её сложить по линии центральных Middle Switch, получается такая трёхуровневая сеть (folded clos):
<img src="https://fs.linkmeup.ru/images/adsm/2/folded_5_stage_clos.png" width="800">

Стоит дать пояснения:
<img src="https://fs.linkmeup.ru/images/adsm/2/2_planes_fabric.png" width="800">

После сворачивания сети Клоза мы получили 4 обособленные группы - их называют <b>POD</b>'ами - <b>Point Of Delivery</b>. В каждой из них свой набор спайнов - это <b>спайны первого уровня</b>.
POD - это универсальная единица при строительстве датацентров - как кубик лего. Можно сказать, при заказе дополнительных мощностей, просто закупают новый POD и подключают его к новой фабрике.
Спайны в одном POD'е далее соединены со спайнами в другом POD'е через <b>спайны второго уровня</b>. Но здесь уже не полная связность: не все спайны первого уровня включены во все спайны второго уровня - напротив, есть разделение по так называемым плоскостям - <b>Plane</b>.
По одному спайну из каждого POD'а выводится в отдельную плоскость, в которой есть свой отдельный набор спайнов второго уровня.
Сделано это не из вредности, а потому что количество портов на спайнах ограничено и сделать топологию полносвязной просто не получится. На иллюстрации выше - ограничение в 4 порта.

И так по числу уровней можно расти дальше.
<hr>


Сеть Клоза по задумке строится из одинаковых элементов, с одинаковым радиксом (количеством портов).
И тут мы имеем ту же самую проблему, что и с <a href="https://linkmeup.ru/blog/312.html#ARCHITECTURE" target="_blank">чипами</a>:
<a href="https://linkmeup.ru/blog/312.html#ARCHITECTURE" target="_blank"><img src="https://habrastorage.org/webt/kc/cd/ze/kccdzeml2g5gfo15qx4ueanhto4.png" width="700"></a>

Чтобы обеспечить неблокируемую (без переподписки) фабрику, аплинки по пропускной способности должны быть такими же, как даунлинки.
И тут появляется нюанс при масштабировании.

Давайте разбираться.
<blockquote>
    Далее предполагаем, что на всех уровнях мы используем одни и те же устройства, на которых только один тип портов и для даунлинков и для аплинков.
</blockquote>

Так, если на коммутаторе всего 2 порта, можно включить всего 1 лиф и 2 машины (или сколько угодно лифов и всё равно 2 машины).

<img src="https://fs.linkmeup.ru/images/adsm/2/2_ports_1_stage.png" width="300">

или

<img src="https://fs.linkmeup.ru/images/adsm/2/2_ports_3_stages.png" width="300">

Если 4 порта, то 4 лифа по 2 даунлинка - 8 машин.
<img src="https://fs.linkmeup.ru/images/adsm/2/4_ports_3_stages.png" width="300">

8 - 8 лифов по 4 даунлинка - и 32 машины.
<img src="https://fs.linkmeup.ru/images/adsm/2/8_ports_3_stages.png" width="400">

Более близкий к реальности сценарий - 32 порта - 32 лифа по 16 даунлинков - 512 машин.
<img src="https://fs.linkmeup.ru/images/adsm/2/32_ports_3_stages.png" width="300">

То есть от числа портов напрямую зависит число машин, которое возможно подключить.
При дальнейшем масштабировании всё больше растёт число спайнов.
Это при трёхуровневой сети Клоза (лиф-спайн-лиф).


Однако если добавить ещё один уровень, то станет дышаться легче:
<img src="https://fs.linkmeup.ru/images/adsm/2/4_ports_5_stages.png" width="300">

Тогда при 4 портах можно подключить 8 лифов и 16 машин, а при 8 - 128.
<img src="https://fs.linkmeup.ru/images/adsm/2/8_ports_5_stages.png" width="300">

32-хпортовые свитчи тогда позволят подключить 8192 хоста - в 16 раз больше, чем в трёхуровневой сети Клоза.
<img src="https://fs.linkmeup.ru/images/adsm/2/32_ports_5_stages.png">


<blockquote>
    Посоветую вот этот простой, но <a href="https://github.com/h8liu/ftree-vis" target="_blank"> изящный инструмент</a>, который рассчитает количество и возможных хостов, и необходимых свитчей, и кабелей.
    Я склонировал репу себе, и вы можете <a href="https://fs.linkmeup.ru/ftree-vis/index.html" target="_blank">поиграться</a> прямо сейчас.

    А ещё детальные вычисления для сети Клоза даны <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part2/" target="_blank">тут</a>.
</blockquote>

Реальный мир как обычно посложнее. Всё же на самом деле мы не используем одни и те же коммутаторы на всех уровнях.
Типичный тор/лиф сегодня в крупных ДЦ - это 48x10/25G даунлинков и 6-8x100G аплинков. 
Типичный спайн - от 32 до 128х100G 

Тут арифметика уже другая. Посчитайте сами. Но очевидно, что подключить в этом случае можно гораздо больше машин.
<hr>

<img src="https://fs.linkmeup.ru/images/adsm/2/squirrels.jpg" width="500">
<a name="GLOSSARY"></a>
<h1>3-Tier, Fat Tree, Clos, Folded Clos, Leaf-Spine?! Аааа!</h1>
Если у вас такая же реакция, то вам в эту главу!
Всё достаточно просто с последними тремя. 
<b>Clos - Топология Клоза</b> - это теоретические выкладки о том, как малыми усилиями построить сеть без блокировок.
<b>Folded Clos - свёрнутая топология Клоза</b> - репрезентация топология Клоза, сложенной пополам, в которой входы - они же выходы, а выходы там же, где входы.
<b>Leaf-Spine</b> - это тот же Folded Clos, реализованный в виде сети датацентра.

А дальше посложнее.
Складывается ощущение что единой терминологии тут не выработано.
Пока я писал эту статью, пришлось 3 раза полностью переписывать некоторые её части, потому что менялось отношение к терминам.
В одних источниках <b>Fat Tree</b> синонимируется с топологией Клоза, в других - с 3-Tier.
Я неоднократно встречал статьи, где современному "Leaf-Spine противопоставляется <i>классический</i> Fat-Tree".
Хотя в той же <a href="https://ru.wikipedia.org/wiki/Fat_Tree" target="_blank">википедии</a> Fat Tree изображён в виде Клоза, или в <a href="https://tools.ietf.org/html/rfc7938">RFC 7938</a> говорится об их синонимичности.

В общем, предлагаю не поддаваться соблазну оставить вопрос без решения. Отныне и навсегда: есть классический <b>3-Tier</b>: Access, Aggregation, Core - а есть <b>Fat-Tree</b> - он же Leaf-Spine.
Но есть и <a href="#OTHERTOPOS" target="_blank">другие</a>.

Принимаю аргументированные претензии в комментариях.

<hr>

<a name="ROUTING"></a>
<h1>Маршрутизация</h1>
Для маршрутизации в Клозе стандартом де-факто стал BGP.
Почему?
На эту тему есть <a href="https://tools.ietf.org/html/rfc7938" target="_blank">целый RFC</a> имени Facebook'a и Arista, где рассказывается, как строить <b>очень крупные</b> сети датацентров, используя BGP. Читается почти как художественный, очень рекомендую для томного вечера.
В нём и про топологии Клоза, и про L3-дизайны, и про BGP в качестве единственного протокола маршрутизации, и даже про OPEX с CAPEX'ом. В общем мне Пётр Лапухов (он один из авторов) идею продал. 

Что такое крупный? Это тысячи стоек в одном датацентре, то есть это тысячи сетевых устройств - не всем IGP по зубам. Даже при разбиении на OSPF-зоны или ISIS (запрещённый в Российской Федерации протокол маршрутизации)-level'ы вопросы уместности здесь Link-State протоколов остаются: 
<ul>
    <li>Почти отсутствующие политики маршрутизации,</li> 
    <li>Очень ограниченные возможности агрегации маршрутов</li>
    <li>Для каждого VRF нужно заводить отдельные процессы</li>
    <li>А если хочется распространять не только IPv4-unicast маршруты?</li>
    <li>LS IGP рассылает обновления на всю зону при любых изменениях топологии, в то время как EBGP на месте выбирает лучший маршрут, и если ничего не меняется, то и помалкивает.</li>
    <li>Ещё одна особенность IGP-протоколов - периодический фладинг LS-информацией. OSPF раз в 30 минут будет рассылать многие тысячи сообщений, напрягая CPU.</li> 
    <li>Учитывая плотность линков на фабрике, одни и те же LSA много раз проходят по сети. LS IGP получается слишком говорливым.</li>
</ul>


При этом от BGP как протокола сигнализации между датацентрами, скорее всего, никуда не денешься.
А BGP супер-масштабируемый, расширяемый протокол, который уже показал свою эффективность на самой крупной сети нашей Солнечной системы.
Десятки и сотни тысяч (<a href="https://nag.ru/news/newsline/105287/zabityiy-do-otkaza-reestr-vozmojno-polojil-revizor-.html" target="_blank">даже миллионы</a>) маршрутов он ест на завтрак.
Как раз то, что нужно. 
Тем более мы сокращаем стек используемых технологий - везде BGP.

Если у вас есть вопросы к сходимости BGP, то тут в наши дни всё тоже неплохо: таймеры можно выкрутить, BFD или BGP Next-hop tracking включить, <a href="https://tools.ietf.org/html/draft-ietf-rtgwg-bgp-pic-05" target="_blank">PIC</a> <a href="https://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html" target="_blank">опять же</a>. Время распространения информации при изменении топологии - примерно такое же, что и у LS IGP. Количество информации тоже как минимум не больше - нет периодической рассылки LSDB - только реальные события инициируют сообщения BGP Update. Кроме того, домен распространения обновлений можно сократить агрегацией маршрутов.


<blockquote>
    <i>Стоит вспомнить, как в середине СДСМ мы наивно смеялись над идеей BGP в качестве IGP, а теперь пришлось к ней обратиться</i>.
    Хотя, безусловно, у BGP есть свои недостатки. По сравнению с LS IGP это, безусловно, отсутствие автообнаружения соседей. При настройке администратору нужно указать их в конфигурации, соотвественно держать где-то об этом информацию. Хотя в качестве контраргумента можно привести в пример <a href="https://tools.ietf.org/html/draft-acee-idr-lldp-peer-discovery-05" target="_blank">BGP Logical Link Discovery Protocol</a> и <a href="https://cumulusnetworks.com/lp/bgp-ebook/">попытки Mellanox</a> облегчить эту задачу.
    Во-вторых, это вычисление наилучшего маршрута. В BGP оно несколько грубовато - на основе длины AS Path, Local Preference и MED. Учёта стоимости линков нет (AIGP не работает, потому что IGP нет). 
    В случае датацентровых сетей это не звучит большой проблемой - обычно всё-таки все линки одной полосы и одной стоимости. Хотя для проведения работ, например, чтобы вывести узел из эксплуатации не получится просто задрать кост на интерфейсах - нужно через политики пессимизировать маршруты или навешивать <a href="https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-grace-shut.pdf" target="_blank">Gshut community</a>.
</blockquote>

Итак, IGP не рассматривается как протокол для больших датацентров в принципе. BGP - на сегодня стандарт, но со своими ограничениями - главное из них - это возможность блэкхола трафика при агрегации маршрутов. Я об этом напишу чуть <a href="#DRAWBACKS">позже</a>.
<hr>

А есть ли альтернативы?
И да, и нет. 

Последние несколько лет активно развивается <a href="https://tools.ietf.org/html/draft-ietf-rift-rift-01" target="_blank">RIFT - Routing In Fat Trees</a>. 
Все протоколы маршрутизации были разработаны специально для сетей с произвольной топологией и не очень высокой степенью связности между сетевыми узлами. Топология Клоза же напротив является регулярной и плотной.
Умные дядьки вдруг решили разрушить полярный мир LS vs DV и смешали их: LS в сторону спайнов, DV в сторону лифов. Детальная информация распространяется только вверх - не вбок и не вниз.
Таким образом каждый уровень сети знает все детали об уровнях ниже, но не про уровни выше. На лифах получается только дефолт.

Кроме того, есть механизм автоматической дезагрегации, который борется с блэкхолом при отказах линков. В ситуации, которую я привёл в <a href="#DRAWBACKS">секции недостатокв</a>, он начинает анонсировать специфики вниз фабрики, чтобы выбрался путь только через те устройства, которые действительно знают как добраться до адресата.

Это объяснение того, почему альтернативы есть.
А нет их, потому что RIFT на сегодняшний день реализован на полутора моделях устройств. Более того, несмотря на его плюсы, не все эксперты так уж оптимистично настроены по поводу его будущего.
Ну а мы посмотрим.
<hr>

<a name="TUNNELS"></a>
<h1>Вам IP или туннелировать?</h1>
Есть крупные компании, которые используют плоскую сеть и держать все абсолютно ресурсы в анедрлее. 
Но они, скорее, исключение - большинству других нужна изоляция сервисов. Достигается она обычно заведением виртуальной сети.
В цикле АДСМ уже <a href="https://linkmeup.ru/blog/449.html" target="_blank">была большая статья</a> об оверлеях - глубоко забираться не будем.
Оверлейная сеть состоит из двух участков - изолированный неймспейс на машине и изолированная передача трафика по андерлейной сети. 
Второе достигается засчёт инкапсуляции. И их мы знаем много видов:
<ul>
    <li>GRE-туннель</li>
    <li>VXLAN</li>
    <li>MPLS</li>
    <li>L3VPN</li>
    <li>GENEVE</li>
</ul>
Сеть андерлей при этом предоставляет базовую маршуртизацию между хостами.
Практически любой из видов инкапсуляции может начинаться на хосте или на ToR-коммутаторе - это не так уж важно.
А вот что важно, так это то, что они отличаются друг от друга заголовками, по которым происходит маршрутизация - IP или MPLS. Например, VXLAN упаковывает данные в заголовки UDP и IP, Tungsten Fabric - в GRE и IP, а MPLS BGP L3VPN - в MPLS.
От этого зависит, какие протоколы должны поддерживаться фабрикой - достаточно ли только IP или нужно и MPLS.
И насколько бы ни был MPLS гибок, удобен и во всех отношениях прекрасен, с ним на датацентровой фабрике появляются нюансы.

<ul>
    <li>Необходимо поддерживать ещё какие-то протоколы (скорее всего, это BGP Labeled Unicast) и дополнительные сущности (LFIB, состояния LSP).</li>
    <li>В условиях ECMP <a href="https://linkmeup.ru/blog/482.html#MPLS_ECMP_GROUPS" target="_blank">MPLS ведёт себя из рук вон плохо</a>. Он натурально опасен - нужно тщательно следить за ресурсами ECMP-групп.</li>
    <li>Как ни крути, а LFIB забирает на себя ресурсы чипов (TCAM и RAM), хотя тут надо постараться, чтобы до их ограничений ещё добраться.</li>
    <li>Когда стандартом де факто в сетях ДЦ стал VXLAN Broadcom выпилил из некоторых своих чипов поддержку MPLS вовсе. Учитывая, что он до сих пор является монополистом, выбор оборудования с поддержкой MPLS становится крайне невелик.</li>
</ul>

Таким образом оверлею в датацентровых сетях быть, а вот MPLS'у скорее нет.

Из набирающих шум баззвордов стоит вспомнить SRv6, который позволяет махом решить многие вопросы, но оставим это пока для лабораторных исследований.
<hr>


<a name="DRAWBACKS"></a>
<h1>Минусы сети Клоза</h1>
Да, да, они есть, несмотря на всю простоту и красоту подхода.
<b>Во-первых</b>, это количество необходимых <a href="https://fs.linkmeup.ru/images/adsm/2/32_ports_5_stages.png" target="_blank">кабелей, портов и свитчей</a>. Каждый новый лиф должен быть подключен во все спайны. А каждый новый спайн - во все лифы. Хотя это является естественным свойством этой топологии, и архитектор к нему заведомо готов.
<b>Во-вторых</b>, максимальное количество хостов определено уже на моменте проектирования. Если их понадобится больше, то нужно строить новый кластер/датацентр и соединять его с первым. Очевидно, что это будет две разных инсталляции со сравнительно узкими линками между друг другом, а это автоматически означает, что приложения, активно нагружающие сеть, тоже должны запускаться отдельно в этих кластерах. 
Если всё-таки для приложений важно, чтобы это был один кластер, то нужно заранее продумывать какое число уровней фабрики будет нужно. А это всегда дополнительные расходы, причём весьма значительные. И отсюда вытекает следующий недостаток:
<b>В-третьих</b>, фабрика, основанная на топологии Клоза, должна быть построена почти в полном объёме ещё до того, как туда запустят первые сервисы.

<img src="https://fs.linkmeup.ru/images/adsm/2/not_fully_constructed_fabric.png" width="800">

Например, тут, вы не можете не построить одну плоскость, как не можете не поставить один из спайнов в плейне. С одной стороны - это деградация пропускной способности, с другой - отсутствие резервирования, если речь идёт о двух устройствах.
Я немного лукавлю, конечно: не всегда нужна сразу вся запланированная пропускная способность, поэтому можно запланировать 32 плейна, а начать с 4 и плавно добавлять их по мере необходимости. 
Но! При таких масштабах совсем иначе стоит вопрос коммутаций - это на самом деле непростая задача <a href="https://youtu.be/U86Xjx1rcHY?t=2717" target="_blank">проектирования и экономики</a>. Многие сотни километров оптики должны быть проложены ещё в момент строительства. И на этом фоне стоимость самих коробок может быть гомеопатически малой.
То есть к моменту запуска нагрузки, сеть должна быть уже практически полностью построена.

<b>В-четвёртых</b>, впрочем это сложно назвать недостатком, скорее особенностью топологии Клоза - это маршрутизация.
При движении на север есть много путей, а на юг - всегда только один.

Мы тут используем горячо любимый BGP. И он хорош. Но сценарии отказа и распространения маршрутной информации нужно продумывать очень тщательно. 
На большой сети мы не можем позволить везде бегать спецификам с торов. Во-первых, их может быть слишком много, что может вызывать исчерпание ресурсов FIB (например, <a href="https://linkmeup.ru/blog/482.html#ECMP_GROUPS" target="_blank">ECMP-групп</a>). Во-вторых, малейшие флапы линков будут вызывать наводнение всей сети обновлениями.
Агрегирование маршрутов позволяет решить обе сложности, однако создаёт новую: блэкхолинг трафика.
Вот ситуация, которая к нему приведёт:
<img src="https://fs.linkmeup.ru/images/adsm/2/blackholes_and_reveleations.png" width="800">

За Leaf 00 живёт хост 10.0.0.1.
Leaf 00 анонсирует маршрут 10.0.0.0/24 на Level 1 Spine 00 (и на Spine 01, конечно).
Рвётся линк между Leaf 00 и Spine 00. Маршрут 10.0.0.0/24 исчезает.
Leaf 01 продолжает анонсировать маршрут 10.0.1.0/24 на Spine 00, зажигая агрегат 10.0.0.0/20.
Level1 Spine 00 анонсирует этот агрегат на Level2 Spine 00. А тот его переанонсирует в другой POD.
POD1 имеет 4 маршрута 10.0.0.0/20 - через спайны второго уровня: Spine 00, 01, 02, 03.
Хосты отправляют трафик на DIP: 10.0.0.1. Этот трафик балансируется по 4 путям.
Тот трафик, который приходит на Level 2 Spine 00 далее попадает только на Level 1 Spine 00.
Однако на нём нет маршрута к специфику 10.0.0.0/24, а агрегат указывает в Null, поскольку на этом устройстве и зарождён.
То есть при наличии нескольких рабочих путей в этом случае четверть трафика будет теряться. 

С агрегацией нужно быть предельно аккуратным. 
<hr>
<a name="OTHERTOPOS"></a>
<h1>Альтернативные топологии</h1>
И я тут не про звезду и full mesh. 
Существуют топологии, в которых можно построить сеть с той же полосой и на то же количество серверов, что и Клоз, но обойтись меньшим количеством свитчей и кабелей.

Мы точно не будем здесь развивать эту тему, я просто дам вам отправные точки:
<ul>
    <li><a href="https://www.researchgate.net/publication/220771890_Flattened_butterfly_a_cost-efficient_topology_for_high-radix_networks" target="_blank">Flattened butterfly</a></li>
    <li><a href="https://people.inf.ethz.ch/asingla/papers/jellyfish-nsdi12.pdf" target="_blank">Jellyfish</a></li>
    <li><a href="https://www.researchgate.net/publication/313341364_Dragonfly_Low_Cost_Topology_for_Scaling_Datacenters">Dragonfly</a></li>
    <li>Любопытный сравнительный <a href="https://nnov.hse.ru/data/2017/07/13/1170958979/The%20comparative%20analysis%20of%20big%20networks.pdf" target="_blank">обзор разных топологий</a>.</li>
</ul>
Стоит лишь помнить, что это топологии, для которых ещё не разработаны протоколы Control Plane'а.
Однако говорят, что их уже используют в HPC - если у вас есть тому свидетельства, буду рад добавить в статью. 
<hr>

<a name="DEVICES"></a>
<h1>Выбор оборудования</h1>
Тут, наверняка, нет универсального совета. 
Но мы попробуем порассуждать.
И стоит вспомнить о промежуточном дизайне сети ДЦ, который был между 3-Tier и полноценным многоуровневым Клозом, построенным из однотипных элементов.
Как только начали переходить на L3, сразу появился соблазн использовать ECMP и ставить много (больше, чем два) агрегирующих устройств.
Так, например, в Facebook появились кластера - набор стоек с ToR'ами, подключенными в четыре кластерных свитча.
Кластера подразумевали сотни стоек, то есть сотни портов на кластерных свитчах.
В те годы это могли быть только большие модульные шкафы с линейными картами, фабриками коммутации, отдельными платами управления, выжирающие электропитание и греющие горячий коридор.
С такими модульными коммутаторами есть несколько проблем:
<b>Во-первых</b>, такие устройства имеют сложную внутреннюю архитектуру. Так каждая линейная карта имеет как минимум один чип коммутации (а то и два), свой чип Traffic Management, подключение к общей шине.
Фабрики коммутации сами по себе сложны по количеству взаимосвязанных компонентов, хотя и не выполняют интеллектуальные функции. 
Над всем этим есть VoQ, какие-то общие на коробку диспетчеры.
Control Plane работает на управляющей плате и должен синхронно и гарантировано доставить данные в Data Plane всех линейных карт.
Эта архитектура, как и управляющий софт, закрыты и крайне сложны для отладки.
<b>Во-вторых</b>, именно из-за наличия такого количества чипов, вероятность того, что где-то начнутся потери - она не только есть, но такие ситуации, увы, часто случаются.
Самое в них неприятное, что они тяжело отлавливаются, потому что никак не дают о себе знать - в ошибках на интерфейсах всё чисто, дропы в очередях QoS тоже по нулям, в логах пусто. Да и связность не полностью пропадает - она или с потерями или всё в целом нормально, но отсутствует связность между конкретными префиксами. Учитывая количество путей, по которым раскладывается трафик, отследить где именно и почему что-то теряется становится задачей нетривиальной и зачастую сводящейся к тому, чтобы поочерёдно перезагружать подозрительные устройства (поднимите руки, кому не приходилось этого делать).

<blockquote>
    Вызваны они бывают как внешними факторами, так и внутренними.
    На моей памяти было два случая, когда происходило замятие контактов на задней части платы, которая вставляется в общую шину. Один раз достаточно сильный инженер засадил линейку в шасси, не заметив, что к контактам прилипла гайка - не только замял контакты но и сжёг шину замыканием.
    Бывают наоборот слабые инженеры, которые недовставят плату, и часть SerDes'ов не работают - отсюда и частая рекомендация TAC - вытащить и вставить.

    Гораздо более частая причина - неверно запрограммированный FIB - это уже косяк ПО. Когда запись прогружается из RIB в FIB, что-то может пойти не так, и запись не доедет до чипа.
    Спрашивайте своего вендора, как проверять актуальность FIB, причём именно аппаратного - есть ли интересующая вас запись в чипе, а не просто в оперативной памяти линейной карты.

    В наше время девятинанометровых техпроцессов случаются ситуации, когда бит в ячейке памяти меняет своё значение почти спонтанно. Условно, альфа-частица во время солнечной активности или от куска урана, распадающегося на соседнем юните в стойке, может пролететь через чип и привести к этому.
    Это повреждает записанную в ячейке информацию или инструкцию и ломает передачу данных.
</blockquote>

А <b>в-третьих</b>, такие гигантские коробки производит всего несколько вендоров. Само по себе это, возможно, и не является проблемой, но привязанность к производителю может выйти боком.

Так или иначе, через подобный дизайн проходили все.

Модная тенденция этой осени - строить фабрику на пицца-боксах - одночиповых коммутаторах небольшого размера - от одного до четырёх рэк-юнитов.
Это в определённом смысле очень выигрышный подход. С одной стороны он позволяет убрать всю эту скрытую архитектурную сложность модульных шасси - если одночиповая коробка ломается, то ломается целиком, а не частично (впрочем, есть у меня для вас пара историй). С другой - они маленькие, дешёвые и производительные - то, что надо в парадигме горизонтального масштабирования. Очень легко докупается ещё одна коробочка на 64х100G порта в качестве спайна для расширения пропускной способности.
<hr>

И тут как раз происходит смещение с подхода с кластерными свитчами на полноценную 5- (и больше) уровневую топологию Клоза.

Сегодня можно найти фантастические по меркам пятилетней давности штуковины.
Например, на ToR 48х25G+6х100G, как вам?
А 128х100G, умещённые в 4 юнита? 12,8 Тб/с на одном чипе - шутка ли?

До едва ли не прошлого года был здесь нюанс с тем, что производитель чипов для датацнетровых коммутаторов был ровно один - Broadcom, заваливший рынок своими Tomahawk/Tomohawk+/2, Trident1/2/3 и Jericho.

Сегодня стали появляться мощные конкуренты - Mellanox со своими Spectrum'ами, Barefoot с Tophino, Innovium - и стало уже можно выбирать, и даже немного манипулировать.
Некоторые вендоры делают ставку на чипы собственного производства.
Но выбор чипа - это уже тема отдельной статьи.
<hr>
<a name="THEEND"></a>
<h1>Заключение</h1>
Картинка, как по мне, получается красивая. Но всегда следует помнить, что и инженеры, и админы, и девопсы, и архитекторы решают задачи бизнеса, а не диктуют как бизнесу подстраиваться под классную сеть, которую они придумали. Поэтому  место найдётся и для топологий Клоза и для 3-Tier и для MSTP в ядре (увы).
Кстати, сеть Клоза на самом деле изобретена не Клозом, а Эдсоном Эрвином на 15 лет раньше, но кого это теперь волнует? :)

<a name="LINKS"></a>
<h1>Полезные ссылки</h1>

<b>Сети Клоза:</b>
<ul>
    <li><a href="https://www.gdt.id.au/~gdt/presentations/2016-07-05-questnet-sdn/papers/bell195303--clos--a-study-of-non-blocking-switching-networks.pdf" target="_blank">Оригинальная публикация <i>A study of non-blocking switching networks by Charles Clos</i></a>.  
    Натурально научный угар. Если не верите, вот вам скринпруф:  
    <img src="https://fs.linkmeup.ru/images/adsm/2/formulas.png" width="500">
    </li>
    <li>Короткая вводная в сеть Клоза: <a href="https://www.networkworld.com/article/2226122/clos-networks-what-s-old-is-new-again.html" target="_blank">Clos Networks: What's Old Is New Again</a>.</li>
    <li>Про сеть Клоза до мельчайших винтиков:
    Demystifying DCN Topologies: Clos/Fat Trees – <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/" target="_blank">Part1</a> и <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part2/" target="_blank">Part2</a>.</li>
    <li>Поверхностный, но очень понятный разбор устройства сети Facebook: <a href="https://www.youtube.com/watch?time_continue=240&v=mLEawo6OzFM" target="_blank">Introduction to Facebook's data center fabric</a>.</li>
</ul>

<b>О вреде God Boxes в ДЦ:</b>
<ul>
    <li><a href="https://comeroutewithme.com/2015/02/28/why-is-clos-spineleaf-the-thing-to-do-in-the-data-center/" target="_blank">Why is Clos (Spine/Leaf) the thing to do in the Data Center?</a></li>
</ul>

<b>Подходы к маршрутизации в L3 ДЦ:</b>
<ul>
    <li>Первый доклад Дмитрия Афанасьева о сетях в ДЦ: <a href="https://www.youtube.com/watch?v=U86Xjx1rcHY&t=1051s" target="_blank">Next Hop 2019: конференция по сетям</a></li>
    <li><a href="https://tools.ietf.org/html/rfc7938" target="_blank">RFC 7938. Use of BGP for Routing in Large-Scale Data Centers</a></li>
    <li><a href="https://cumulusnetworks.com/lp/bgp-ebook/" target="_blank">Cumulus. BGP in the data center</a></li>
    <li>Про оптимизацию BGP: <a href="https://www.ipspace.net/BGP_Convergence_Optimization" target="_blank">BGP Convergence Optimization</a></li>
</ul>

<b>Альтернативные топологии:</b>
<ul>
    <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3926393/" target="_blank">Application of Butterfly Clos-Network in Network-on-Chip</a></li>
    <li><a href="https://www.researchgate.net/publication/220771890_Flattened_butterfly_a_cost-efficient_topology_for_high-radix_networks" target="_blank">Flattened butterfly: a cost-efficient topology for high-radix networks.</a></li>
    <li><a href="https://people.inf.ethz.ch/asingla/papers/jellyfish-nsdi12.pdf" target="_blank">Jellyfish: Networking Data Centers Randomly</a></li>
    <li><a href="https://www.researchgate.net/publication/313341364_Dragonfly_Low_Cost_Topology_for_Scaling_Datacenters" target="_blank">Dragonfly+: Low Cost Topology for Scaling Datacenters</a></li>
</ul>
<hr>

<h1>Спасибы</h1>
<ul>
    <li>Андрею Глазкову aka @glazgoo за вычитку и правки</li>
    <li>Александру Клименко aka @v00lk за то, что перевернул взгляд на топологии</li>
    <li>Артёму Чернобаю за КДПВ</li>
</ul>
<hr>